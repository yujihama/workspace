# Why Language Models Hallucinate | OpenAI

---

## 概要

OpenAIの新しい研究によると、言語モデル（Language Models、LMs）が「幻覚（hallucinations）」を起こす主な理由は、モデルの構造や能力の問題というよりも、**訓練や評価の方法に起因**していることが明らかになりました。具体的には、訓練データや評価基準が「正しい答えを出すこと」に強く報酬を与え、「不確かさを認める」ことには報酬を与えないため、モデルは自信満々にしかし誤った情報を生成しやすくなるのです。OpenAIはこの問題を理解し、GPT-5などで「不確かさを示す」「確認を求める」ような機能を取り入れ、幻覚の発生を減らす取り組みを進めています。

---

## 詳細

### 1. 幻覚（Hallucinations）とは何か？

- **幻覚とは？**  
  言語モデルが自信を持って生成するが、実際には誤っている、または存在しない情報のこと。例として、架空の人物や事実を「真実のように」語ることがあります。

- **なぜ問題か？**  
  幻覚は利用者の信頼を損ね、情報の正確性を求められる場面での実用性を低下させます。

### 2. なぜ言語モデルは幻覚を起こすのか？

#### (a) 訓練と評価の仕組み

- 言語モデルは大量のテキストから「次に来る単語を予測する」タスクで訓練されます。  
- 一般に「正しい答え・正確な情報」がモデルに報酬として与えられますが、「分からない」「不確かである」という選択肢を認める報酬が少ない（もしくはない）ため、  
- モデルは特に分からない時でも、次に来る可能性の高い単語（＝もっともらしい答え）を生成することを優先します。これが「幻覚」を生む要因です。

#### (b) 「正解を出させる」ことが重視される評価

- 従来の評価方法は「正解の答えとどれだけ一致しているか」を重視しているため、モデルは「推測でも答える」動機付けが強い。  
- それにより「不確かさの表明」や「回答を見送る」ことが奨励されず、幻覚が増加します。

#### (c) 幻覚は「次の単語予測」という基本タスクの副産物

- 言語モデルは単語の連鎖の統計的規則性を模しており、「意味の真偽」を理解しているわけではありません。  
- したがって、単語列としてもっともらしく見えるが、実際には誤った情報を選ぶことがあります。

### 3. 幻覚の軽減に向けたアプローチ

- **不確実性の表明を促す**  
  モデルに「わからない」「確認が必要」という選択肢を持たせることで、誤りを減らす試み。

- **GPT-5の改良**  
  GPT-5は特に「推論時の幻覚」が大幅に減っており、不確かさ表明の実装などが効果を上げています。

- **評価方法の見直し**  
  「単に正しい回答かどうか」ではなく、「不確かさを認める姿勢」も評価に加える方向で改善中。

---

## 具体例

### 例1: 幻覚の典型

ユーザーの質問：「世界初の月面着陸は何年ですか？」

- モデルの回答（正しい場合）：「1969年です。」
- モデルの幻覚例：「1975年です。」

この誤答は、モデルが確実な情報を持っていない場合に「もっともらしい単語列」を優先して生成した結果である。

### 例2: 不確かさを示す応答例（改善例）

ユーザーの質問：「新しい科学論文Xについて教えて？」

- モデル回答（改善後）：「すみません、その論文についての情報は持っていません。詳細を教えて頂けますか？」

これにより、誤情報の拡散を抑え、ユーザーとの対話で正確性を保とうとする姿勢を示している。

---

## まとめ

| ポイント                     | 内容                                  |
|----------------------------|-------------------------------------|
| 幻覚の定義                   | 事実と異なる、自信過剰な誤情報の生成     |
| 主因                       | 訓練・評価が「正確な答え」に偏り、不確実性が評価されないため |
| 仕組み                     | 次単語予測タスクによる「もっともらしい」単語列生成が幻覚を生む |
| 改善策                     | 不確かさの表明を促し、評価方法の見直し    |
| GPT-5など最新モデルの特徴    | 幻覚の発生が減少し、「わからない」と答える能力を強化 |

---

## 参考

- [Why language models hallucinate | OpenAI](https://openai.com/index/why-language-models-hallucinate/#main)
- [研究論文PDF](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf)
- OpenAI公式ブログ記事（2024年12月発表）

---

以上がOpenAIによる「言語モデルが幻覚を起こす理由」の解説です。言語モデルの今後の改善には、この「不確かさを正しく扱い評価する」視点が不可欠とされています。